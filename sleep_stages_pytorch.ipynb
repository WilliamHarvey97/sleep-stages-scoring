{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sleep Stage Scoring (on raw EEG signal)\n",
    "\n",
    "What is different from [this version](https://github.com/baroquerock/sleep-stages-scoring):\n",
    "1. Pytorch framework\n",
    "2. Multichannel input\n",
    "3. Different architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New architecture accomodates two channels:\n",
    "\n",
    "<img src=\"img/arch2.png\" width=\"800\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from npzloader import DataLoader as DataLoader_x\n",
    "import torch.utils.data as utils\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "moB1UfGjD7vW"
   },
   "outputs": [],
   "source": [
    "# loading fpz_cz and pz_oz channels\n",
    "loader = DataLoader_x(\"data/fpz_cz\")\n",
    "data_fp, labels = loader.load_data(verbose=0)\n",
    "data_fp = {''.join(k.partition('fpz_cz')[1:]) : v for k, v in data_fp.items()} \n",
    "\n",
    "loader = DataLoader_x(\"data/pz_oz\")\n",
    "data_pz, _ = loader.load_data(verbose=0)\n",
    "data_pz = {''.join(k.partition('pz_oz')[1:]) : v for k, v in data_pz.items()} \n",
    "\n",
    "all_labels = np.hstack(labels.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSFEK863D7ve"
   },
   "outputs": [],
   "source": [
    "# int-to-label map\n",
    "class_dict = {0: \"W\", \n",
    "              1: \"N1\",\n",
    "              2: \"N2\",\n",
    "              3: \"N3\",\n",
    "              4: \"REM\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbmyneTYD7wv"
   },
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uegNojzFD7w2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to randomly select subjects for partial cross-validation and prepare the data for training.\n",
    "Train and test sets should be independent.\n",
    "A patient with is SC413 is excluded from any test set because this patient has only one recording.\n",
    "\"\"\"\n",
    "\n",
    "def get_cv_data(data, labels, n_folds=3, ch='fpz_cz'):\n",
    "  \n",
    "    \n",
    "    random.seed(29)\n",
    "    \n",
    "    keys = list(labels.keys())\n",
    "    keys = list(set([x[:5] for x in keys if '413' not in x]))\n",
    "    random.shuffle(keys)\n",
    "    val_keys = keys[:n_folds]\n",
    "    cv_data = []\n",
    "    \n",
    "    for fold in val_keys:\n",
    "        fold_keys = [fold+'1E0', fold+'2E0']\n",
    "        val_labels = [labels[key] for key in fold_keys]\n",
    "        val_labels = np.hstack(val_labels)\n",
    "        #val_labels = enc.transform(val_labels)\n",
    "               \n",
    "        train_labels = [labels[key] for key in labels.keys() if key not in fold_keys]\n",
    "        train_labels = np.hstack(train_labels)\n",
    "        #train_labels = enc.transform(train_labels)\n",
    "        \n",
    "        if ch:\n",
    "            val_data = [data['{}/{}'.format(ch,key)] for key in fold_keys]\n",
    "            train_data = [data['{}/{}'.format(ch,key)] for key in labels.keys() if key not in fold_keys]          \n",
    "        else:\n",
    "            val_data = [data[key] for key in fold_keys]\n",
    "            train_data = [data[key] for key in labels.keys() if key not in fold_keys]\n",
    "                        \n",
    "        train_data = np.vstack(train_data)\n",
    "        val_data = np.vstack(val_data)\n",
    "        \n",
    "        cv_data.append((train_data, train_labels, val_data, val_labels))\n",
    "    \n",
    "    return cv_data \n",
    "\n",
    "\"\"\"\n",
    "Function to slide on array with a moving window of size n\n",
    "\"\"\"\n",
    "\n",
    "def _slide(data, labels, n=3):\n",
    "    \n",
    "    #zero-indexing\n",
    "    data = [data[i:i+n] for i, _ in enumerate(data[n:])]\n",
    "    data = [np.vstack(x) for x in data]\n",
    "    data = np.array(data)\n",
    "    \n",
    "    labels = labels[n:]\n",
    "    \n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "\"\"\"\n",
    "Function to do batch processing with _slide()\n",
    "\"\"\"\n",
    "\n",
    "def batch_slide(data_dict, labels_dict, n=3):\n",
    "    \n",
    "    keys = [(x,y) for x,y in zip(data_dict, labels_dict)]\n",
    "    data = [_slide(data_dict[x], labels_dict[y], n=n) for x,y in keys]\n",
    "    \n",
    "    data_keys, label_keys = zip(*keys)\n",
    "    data, labels = zip(*data)\n",
    "    \n",
    "    data_dict = {k:v for k,v in zip(data_keys, data)}\n",
    "    data_dict = OrderedDict(sorted(data_dict.items()))\n",
    "    \n",
    "    labels_dict = {k:v for k,v in zip(label_keys, labels)}\n",
    "    labels_dict = OrderedDict(sorted(labels_dict.items()))\n",
    "        \n",
    "    return data_dict, labels_dict \n",
    "  \n",
    "  \n",
    "\"\"\"\n",
    "Function to merge channels into single recording.\n",
    "axis = 0: merge channels along the first dimension - (3000,1) and (3000,1) --> (6000,1)\n",
    "axis = 0: merge channels along the second dimension - (3000,1) and (3000,1) --> (3000,2)\n",
    "\"\"\"  \n",
    "    \n",
    "def merge_channels(channels, axis=0):\n",
    "    \n",
    "    # number of records should be the same in all channels\n",
    "    assert len(set([len(r) for r in channels])) == 1\n",
    "    \n",
    "    keys = [ch.keys() for ch in channels]\n",
    "    \n",
    "    merged = {}\n",
    "    \n",
    "    for records in zip(*keys):\n",
    "        \n",
    "        # assert that all recordings are for the same patient\n",
    "        assert len(set([r.split('/')[1] for r in records])) == 1\n",
    "        vals = [ch[r] for r, ch in zip(records, channels)]\n",
    "        if axis:\n",
    "            stacked = np.dstack(vals)\n",
    "        else:\n",
    "            stacked = np.hstack(vals)\n",
    "        patient = records[0].split('/')[1]\n",
    "        merged[patient] = stacked\n",
    "        \n",
    "    merged = OrderedDict(sorted(merged.items()))\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xb01HsHBoraY"
   },
   "outputs": [],
   "source": [
    "data_all = merge_channels([data_fp, data_pz], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IEsk57FID7xH"
   },
   "outputs": [],
   "source": [
    "# getting cross-valivation data\n",
    "# note: n_folds is not a number of folds in traditional sense\n",
    "# the model is trained on 19 patients and validated on 1,\n",
    "# so n_folds is a number of patients for validation\n",
    "# to perform full cross-validation n_folds should be equal to 19 \n",
    "# (not 20, because for SC413 patient there is only one record)\n",
    "cv_data = get_cv_data(data_all, labels, ch=None, n_folds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dnzl90CrD7yY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Otf2uGzLD7yi"
   },
   "outputs": [],
   "source": [
    "def small_conv(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs//2, stride=fs//16, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=8, stride=8),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=8, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "def big_conv(fs): \n",
    "    return nn.Sequential(\n",
    "            nn.Conv1d(1, 64, kernel_size=fs*4, stride=fs//2, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.3),\n",
    "            nn.MaxPool1d(kernel_size=4, stride=4),\n",
    "            \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU(), \n",
    "            nn.Conv1d(64, 64, kernel_size=6, padding=2),\n",
    "            nn.ReLU())\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_cnn_dense=256, fs=100, num_classes=5):\n",
    "      \n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1_fz = small_conv(fs)        \n",
    "        self.layer2_fz = big_conv(fs)\n",
    "\n",
    "        self.layer1_pz = small_conv(fs)        \n",
    "        self.layer2_pz = big_conv(fs)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(124, n_cnn_dense),\n",
    "            nn.ReLU(),            \n",
    "            nn.MaxPool1d(kernel_size=4, stride=4))\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(2048, num_classes),\n",
    "            nn.LogSoftmax(dim=1))\n",
    "        \n",
    "                \n",
    "        \n",
    "    def forward(self, channels):\n",
    "      \n",
    "        ch1 = channels[:, 0, :].unsqueeze(1)\n",
    "        ch2 = channels[:, 1, :].unsqueeze(1)\n",
    "        \n",
    "        out1_1 = self.layer1_fz(ch1)\n",
    "        out2_1 = self.layer2_fz(ch1)\n",
    "        \n",
    "        out1_2 = self.layer1_pz(ch2)\n",
    "        out2_2 = self.layer2_pz(ch2)        \n",
    "        \n",
    "        out = torch.cat((out1_1, out2_1, out1_2, out2_2), dim=2)\n",
    "        out = self.fc1(out)\n",
    "        s = out.size()[0]\n",
    "        out = out.view(s, -1)\n",
    "        out = self.fc2(out)\n",
    "       \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTBFZaDKne0p"
   },
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "train_data, train_labels, val_data, val_labels = cv_data[3]\n",
    "\n",
    "train = utils.TensorDataset(torch.from_numpy(np.swapaxes(train_data,1,2)), torch.from_numpy(train_labels).long())\n",
    "train_loader = utils.DataLoader(train, batch_size=64, shuffle=True)\n",
    "\n",
    "test = utils.TensorDataset(torch.from_numpy(np.swapaxes(val_data,1,2)), torch.from_numpy(val_labels).long())\n",
    "test_loader = utils.DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "dataloaders = {'train': train_loader, 'valid': test_loader}\n",
    "dataset_sizes = {'train': len(train), 'valid': len(test)}\n",
    "#x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40FA0Mt-qQt8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PJyvQdmkN_aX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xizp4eJJ5kYy"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criteria, optimizer, scheduler, num_epochs=25, device='cuda'):\n",
    "    model.to(device)\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "   \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criteria(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMXmk4v1CARM"
   },
   "outputs": [],
   "source": [
    "criteria = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "sched = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)\n",
    "eps=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1806
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 368141,
     "status": "ok",
     "timestamp": 1546889587450,
     "user": {
      "displayName": "Tatiana Gaponova",
      "photoUrl": "https://lh5.googleusercontent.com/-gUJ79WnHrqY/AAAAAAAAAAI/AAAAAAAAQis/zrKck2fIipY/s64/photo.jpg",
      "userId": "17758316458603777674"
     },
     "user_tz": -60
    },
    "id": "Ns_SuBUhCAgP",
    "outputId": "cd0bfbcd-e4c5-49b6-cbae-644fa8493fb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda!\n",
      "Epoch 1/20\n",
      "----------\n",
      "train Loss: 0.9402 Acc: 0.6585\n",
      "valid Loss: 0.7046 Acc: 0.7204\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "train Loss: 0.7796 Acc: 0.7028\n",
      "valid Loss: 0.6260 Acc: 0.7691\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "train Loss: 0.8006 Acc: 0.6943\n",
      "valid Loss: 0.6130 Acc: 0.7873\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "train Loss: 0.7940 Acc: 0.6946\n",
      "valid Loss: 0.7595 Acc: 0.7031\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "train Loss: 0.6948 Acc: 0.7393\n",
      "valid Loss: 0.6100 Acc: 0.7655\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "train Loss: 0.6605 Acc: 0.7546\n",
      "valid Loss: 0.5572 Acc: 0.7901\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n",
      "train Loss: 0.6349 Acc: 0.7651\n",
      "valid Loss: 0.5540 Acc: 0.7860\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n",
      "train Loss: 0.6190 Acc: 0.7705\n",
      "valid Loss: 0.5440 Acc: 0.7933\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n",
      "train Loss: 0.6051 Acc: 0.7777\n",
      "valid Loss: 0.5354 Acc: 0.7978\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n",
      "train Loss: 0.5986 Acc: 0.7781\n",
      "valid Loss: 0.5159 Acc: 0.8078\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n",
      "train Loss: 0.5961 Acc: 0.7797\n",
      "valid Loss: 0.5298 Acc: 0.7974\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n",
      "train Loss: 0.5946 Acc: 0.7800\n",
      "valid Loss: 0.5194 Acc: 0.8005\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n",
      "train Loss: 0.5922 Acc: 0.7801\n",
      "valid Loss: 0.5199 Acc: 0.7996\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n",
      "train Loss: 0.5932 Acc: 0.7813\n",
      "valid Loss: 0.5204 Acc: 0.8024\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n",
      "train Loss: 0.5930 Acc: 0.7799\n",
      "valid Loss: 0.5193 Acc: 0.8001\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n",
      "train Loss: 0.5926 Acc: 0.7806\n",
      "valid Loss: 0.5197 Acc: 0.8024\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n",
      "train Loss: 0.5926 Acc: 0.7800\n",
      "valid Loss: 0.5198 Acc: 0.8024\n",
      "\n",
      "Epoch 18/20\n",
      "----------\n",
      "train Loss: 0.5916 Acc: 0.7824\n",
      "valid Loss: 0.5197 Acc: 0.8019\n",
      "\n",
      "Epoch 19/20\n",
      "----------\n",
      "train Loss: 0.5914 Acc: 0.7819\n",
      "valid Loss: 0.5194 Acc: 0.8024\n",
      "\n",
      "Epoch 20/20\n",
      "----------\n",
      "train Loss: 0.5902 Acc: 0.7820\n",
      "valid Loss: 0.5192 Acc: 0.8024\n",
      "\n",
      "Training complete in 6m 8s\n",
      "Best val Acc: 0.807832\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using {}!\".format(device))\n",
    "model_ft = train_model(model, criteria, optimizer, sched, eps, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sleep_stages_pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
